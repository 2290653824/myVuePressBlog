## 1. 排查小案例



### Young GC 频繁

之前有个任务会频繁地重复调用一个接口。所以用guava cache做了一个简单的内存缓存。结果上线后发现经常收到Young GC频繁的告警，时间跟这个任务的启动时间也比较吻合。

通过监控看到的GC图大概是这样：

![image-20230718212631442](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230718212631442.png)^案例1^

可以看到，Young GC的次数会在某一个时间点飙升。同时伴随着Old区域内存快速升高，最后会触发一次Full GC。

根据这个情况，可以肯定的是由于本次代码改动引起的。通过Heap Dump分析后发现，占用内存较大的是一个guava cache的Map对象。

查找代码发现，使用guava cache的时候，没有设置最大缓存数量和弱引用，而是设置了一个几分钟的过期时间。而这个任务的量又比较大，到线上后很快就缓存了大量的对象，导致频繁触发Young GC，但又由于有引用GC不掉（这个从Survivor区的内存大小图像可以推测），所以慢慢代数比较多的对象就晋升到了老年代，后面老年代内存到达一定阈值引发Full GC。

后面通过设置最大缓存数量解决了这个问题。又积累了一个宝贵的经验，完美！



###  Young GC和Old GC都频繁

在线上灰度环境中发现收到Young GC和Old GC频繁的告警。监控看到的GC图大概长这样：

![image-20230718212623092](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230718212623092.png)

根据GC图大概可以看出来，Young GC和Old GC都非常频繁，且每次都能回收走大量的对象。那可以简单地推测：确实是产生了大量的对象，且极有可能有一部分大对象。小对象引发的Young GC频繁，而大对象引发了Old GC频繁。

排查下来也是一段代码引起的。对于一个查询和排序分页的SQL，同时这个SQL需要join多张表，在分库分表下，直接调用SQL性能很差，甚至超时。于是想了个比较low的办法：查单表，把所有数据查出来，在内存排序分页。用了一个List来保存数据，而有些数据量大，造成了这个现象。用Heap Dump分析，也印证了这个猜测，List类型的对象占用了大量的空间。



### **接口线程池满和Full GC**

这是一个报接口线程池满的问题。但每次都会在同一时间Full GC。监控图大概长这样：

![image-20230718212701999](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230718212701999.png)^案例3^

从时间上来看，先是Java线程数量飙升，然后触发Full GC。后面重启后，Java线程数量恢复正常水位。

这里涉及到一个冷知识：一个Java线程默认会占用多少内存？

> ->
>
> 这个参数可以控制：-XX:ThreadStackSize。在64 位的Linux下, 默认是1 MB（这个说法也不全对，还是取决于栈深度）。Java 11对这个有一个优化，可以减少内存占用。详情可以参考这篇文章：https://dzone.com/articles/how-much-memory-does-a-java-thread-take
>
> <-

排查下来根因是这个应用还是使用的Log4j 1，而Log4j 1有性能问题，在高并发下，下面这段代码的同步块可能会引起大量线程阻塞：

```
void callAppenders(LoggingEvent event) {
    int writes = 0;

    for(Category c = this; c != null; c=c.parent) {
        // Protected against simultaneous call to addAppender, removeAppender,...
        synchronized(c) {
            if(c.aai != null) {
                writes += c.aai.appendLoopOnAppenders(event);
            }
            if(!c.additive) {
                break;
            }
        }
    }

    if(writes == 0) {
        repository.emitNoAppenderWarning(this);
    }
}
```

解决办法就是减少日志打印，升级日志框架到Log4j 2或者Logback。



### 应用启动时Full GC频繁

这个是较早的一个案例了，GC图已经找不到了。

但引发Full GC频繁的大概就这几种可能性：

- 调用System.gc()
- Old区空间不足
- 永久代/元空间满

根据代码和GC图排除了前面两种可能性，那就是元空间满了。在Java 8中，`XX:MaxMetaspaceSize`是没有上限的，最大容量与机器的内存有关；但是`XX:MetaspaceSize`是有一个默认值的：21M。而如果应用需要进元空间的对象较多（比如有大量代码），就会频繁触发Full GC。解决办法是可以通过JVM参数指定元空间大小：`-XX:MetaspaceSize=128M`。



## 2.大型调优问题

### 背景

我们都知道 JVM 分为了新生代和老年代，并且我们在启动应用的时候都会配置对应的参数，为应用程序运行的 JVM 调整内存大小。但我们都知道，很多时候我们都只是大致估计一个数，随便填填，然后就上线了。

作者所在的公司同样存在这种情况，JVM 内存大小基本上都设得挺大的，毕竟内存大总比内存溢出好，因此就造成了不少的内存浪费。所以作者收到的任务就是对所有的应用进行一次排查，调整合适的内存参数，优化 JVM 的性能。

### 调优实战

要对应用进行 JVM 性能调优，那么首先得知道其运行的情况。这就像去医院看医生，去开药之前需要医生先望闻问切一样。在 Java 中，有很多方式可以观察到 JVM 的内部情况，例如 JDK 提供的各种命令工作。作者所在公司使用的是 Prometheus 进行监控，因此我们可以直接在 Prometheus 上看到应用的 JVM 运行情况。

Prometheus 面板中与 JVM 相关的主要有四块内容：JVM Misc、JVM Memory Pools（Heap）、JVM Memory Pools（Non-Heap）、Garbage Collection。其中与我们此次较为相关的主要是：JVM Memory Pools（Heap）和 Garbage Collection。

JVM Memory Pools（Heap） 展示 JVM 堆内存的使用情况，主要包括了新生代的 Survivor 区、Eden Space 区、老年代。

![image-20230719220534324](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220534324.png)

Garbage Collection 展示 JVM 的垃圾回收情况，主要包括垃圾回收频率（ops 表示一秒回收几次，一般 0.5 是比较合理的值）、每次 GC 停顿时长（一般 80ms 以下是合理值）、分配到新生代/晋升老年代的内存。

![image-20230719220543397](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220543397.png)

我们要进行 JVM 性能优化，那么最简单的一个方法就是观察 Garbage Collection 的 GC 频率以及停顿时间，我们大致就能判断出应用的内存利用效率。之后根据这两个值的实际情况，将其调整到合理的范围内，提高 JVM 的利用率。

如果一个应用的 GC 频率只有 0.02，即每秒 GC 0.02 次，那么需要 50 秒才 GC 一次，那么其 GC 频率是很低的。这时候很可能是分配了较大的新生代空间，这使得其很久才需要 GC 一次。这时候我们再看看其停顿时间，如果停顿时间也很短的话，那我们就可以判定该应用的内存有优化的空间。

在这种情况下，一般都是缩小分配的新生代的空间。新生代空间一旦变小了，那么其分配完的时间就会缩减。一旦空间被分配完，那么就会启动进行 GC 操作。**我们就是通过调整 JVM 的内存空间，来提高 GC 的频率，从而使其处于一个合理的空间。**

**在进行内存空间调整的时候，为了避免内存剧烈波动导致的问题，一般我们都是小步快跑地一点点调整。先调整一点试一试，没太大问题之后再调整到目标值。** 毕竟是生产环境，要是出了什么叉子，那就得提桶跑路了，还是谨慎为好！

看到这里，想必大家应该也知道怎么做了。接下来无非就是调整 JVM 内存空间的三个参数（-Xmx -Xms -Xmn），使 GC 频率与 GC 停顿时间处于合理的区间。

### 应用层面优化

除了 GC 频率、GC 停顿时间，我们还能从应用的类型来分析 JVM 的内存消耗情况。

例如对于接口类型的系统来说，很多请求都是 1 秒中之内就结束。对于这种类型的请求，他们进入应用时会分配内存，结束时内存就会立刻被回收，留存下来的对象很少。这种应用的 JVM 内存情况大概是这样的：新生代消耗比较大，并且随着周期性回收内存，但老年代的内存消耗则更小。对于那些持续性处理的应用，例如持续时间长的应用处理。因为其存活时间较久，所以可能会有更多的对象晋升到老年代，因此老年代的内存消耗就比较大。

通过观察 JVM 年轻代与老年代的内存消耗情况，再结合应用本身的特性，我们可以发现应用中不合理的地方，再对应用进行针对性的优化。例如：应用某个地方每次都会存储大量的临时数据到内容中，这样就造成了 JVM 可能爆发 GC，从而导致应用卡顿。

### 总结

总结一下本篇文章的调优方法：**通过观察 GC 频率和停顿时间，来进行 JVM 内存空间调整，使其达到最合理的状态。调整过程记得小步快跑，避免内存剧烈波动影响线上服务。** 这其实是最为简单的一种 JVM 性能调优方式了，可以算是粗调吧。但 JVM 性能调优还有更多、更详细的参数，后续有机会我们再聊聊。

此外，通过观察 JVM 年轻代与老年代的情况，也可以帮助我们对应用进行针对性的优化，从而提升应用本身的性能。

如果你之前没了解过 JVM 的基础理论知识，那么你可能看不懂这篇文章。那么我推荐你看看我的「JVM 基础入门系列」，文章由浅入深、循序渐进，可以让你对 JVM 有个感性的理解。看完之后再来看这篇文章，你肯定有种豁然开朗的感觉！

## 3. young GC问题排查

在高并发下，Java程序的GC问题属于很典型的一类问题，带来的影响往往会被进一步放大。不管是「GC频率过快」还是「GC耗时太长」，由于GC期间都存在Stop The World问题，因此很容易导致服务超时，引发性能问题。

我们团队负责的广告系统承接了比较大的C端流量，平峰期间的请求量基本达到了上千QPS，过去也遇到了很多次GC相关的线上问题。

这篇文章，我再分享一个更棘手的Young GC耗时过长的线上案例，同时会整理下YGC相关的知识点，希望让你有所收获。内容分成以下2个部分：

- 从一次YGC耗时过长的案例说起
- YGC的相关知识点总结

### 从一次YGC耗时过长的案例说起

今年4月份，我们的广告服务在新版本上线后，收到了大量的服务超时告警，通过下面的监控图可以看到：超时量突然大面积增加，1分钟内甚至达到了上千次接口超时。下面详细介绍下该问题的排查过程。

![image-20230719220551786](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220551786.png)

#### 检查监控

收到告警后，我们第一时间查看了监控系统，立马发现了YoungGC耗时过长的异常。我们的程序大概在21点50左右上线，通过下图可以看出：在上线之前，YGC基本几十毫秒内完成，而上线后YGC耗时明显变长，最长甚至达到了3秒多。

![image-20230719220600276](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220600276.png)

由于YGC期间程序会Stop The World，而我们上游系统设置的服务超时时间都在几百毫秒，因此推断：是因为YGC耗时过长引发了服务大面积超时。
按照GC问题的常规排查流程，我们立刻摘掉了一个节点，然后通过以下命令dump了堆内存文件用来保留现场。
jmap -dump:format=b,file=heap pid
最后对线上服务做了回滚处理，回滚后服务立马恢复了正常，接下来就是长达1天的问题排查和修复过程。

#### 确认JVM配置

用下面的命令，我们再次检查了JVM的参数

```
ps aux | grep "applicationName=adsearch"
-Xms4g -Xmx4g -Xmn2g -Xss1024K 
-XX:ParallelGCThreads=5 
-XX:+UseConcMarkSweepGC 
-XX:+UseParNewGC 
-XX:+UseCMSCompactAtFullCollection 
-XX:CMSInitiatingOccupancyFraction=80
```

可以看到堆内存为4G，新生代和老年代均为2G，新生代采用ParNew收集器。
再通过命令 jmap -heap pid 查到：新生代的Eden区为1.6G，S0和S1区均为0.2G。
本次上线并未修改JVM相关的任何参数，同时我们服务的请求量基本和往常持平。因此猜测：此问题大概率和上线的代码相关。

#### 检查代码

再回到YGC的原理来思考这个问题，一次YGC的过程主要包括以下两个步骤：

> 1、从GC Root扫描对象，对存活对象进行标注
> 2、将存活对象复制到S1区或者晋升到Old区

根据下面的监控图可以看出：正常情况下，Survivor区的使用率一直维持在很低的水平（大概30M左右），但是上线后，Survivor区的使用率开始波动，最多的时候快占满0.2G了。而且，YGC耗时和Survivor区的使用率基本成正相关。因此，我们推测：应该是长生命周期的对象越来越多，导致标注和复制过程的耗时增加。

![image-20230719220608837](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220608837.png)

再回到服务的整体表现：上游流量并没有出现明显变化，正常情况下，核心接口的响应时间也基本在200ms以内，YGC的频率大概每8秒进行1次。

很显然，对于局部变量来说，在每次YGC后就能够马上被回收了。那为什么还会有如此多的对象在YGC后存活下来呢？

我们进一步将怀疑对象锁定在：程序的全局变量或者类静态变量上。但是diff了本次上线的代码，我们并未发现代码中有引入此类变量。

#### 对dump的堆内存文件进行分析

代码排查没有进展后，我们开始从堆内存文件中寻找线索，使用MAT工具导入了第1步dump出来的堆文件后，然后通过Dominator Tree视图查看到了当前堆中的所有大对象。

![image-20230719220615875](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220615875.png)

立马发现NewOldMappingService这个类所占的空间很大，通过代码定位到：这个类位于第三方的client包中，由我们公司的商品团队提供，用于实现新旧类目转换（最近商品团队在对类目体系进行改造，为了兼容旧业务，需要进行新旧类目映射）。

进一步查看代码，发现这个类中存在大量的静态HashMap，用于缓存新旧类目转换时需要用到的各种数据，以减少RPC调用，提高转换性能。

![image-20230719220622515](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220622515.png)

原本以为，非常接近问题的真相了，但是深入排查发现：这个类的所有静态变量全部在类加载时就初始化完数据了，虽然会占到100多M的内存，但是之后基本不会再新增数据。并且，这个类早在3月份就上线使用了，client包的版本也一直没变过。

经过上面种种分析，这个类的静态HashMap会一直存活，经过多轮YGC后，最终晋升到老年代中，它不应该是YGC持续耗时过长的原因。因此，我们暂时排除了这个可疑点。

#### 分析YGC处理Reference的耗时

团队对于YGC问题的排查经验很少，不知道再往下该如何分析了。基本扫光了网上可查到的所有案例，发现原因集中在这两类上：

> 1、对存活对象标注时间过长：比如重载了Object类的Finalize方法，导致标注Final Reference耗时过长；或者String.intern方法使用不当，导致YGC扫描StringTable时间过长。
> 2、长周期对象积累过多：比如本地缓存使用不当，积累了太多存活对象；或者锁竞争严重导致线程阻塞，局部变量的生命周期变长。

针对第1类问题，可以通过以下参数显示GC处理Reference的耗时-XX:+PrintReferenceGC。添加此参数后，可以看到不同类型的 reference 处理耗时都很短，因此又排除了此项因素。

![image-20230719220629198](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220629198.png)

#### 再回到长周期对象进行分析

再往后，我们添加了各种GC参数试图寻找线索都没有结果，似乎要黔驴技穷，没有思路了。综合监控和种种分析来看：应该只有长周期对象才会引发我们这个问题。
折腾了好几个小时，最终峰回路转，一个小伙伴重新从MAT堆内存中找到了第二个怀疑点。

![image-20230719220637906](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220637906.png)

从上面的截图可以看到：大对象中排在第3位的ConfigService类进入了我们的视野，该类的一个ArrayList变量中竟然包含了270W个对象，而且大部分都是相同的元素。
ConfigService这个类在第三方Apollo的包中，不过源代码被公司架构部进行了二次改造，通过代码可以看出：**问题出在了第11行，每次调用getConfig方法时都会往List中添加元素，并且未做去重处理**。

![image-20230719220645842](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220645842.png)

我们的广告服务在apollo中存储了大量的广告策略配置，而且大部分请求都会调用ConfigService的getConfig方法来获取配置，因此会不断地往静态变量namespaces中添加新对象，从而引发此问题。

至此，整个问题终于水落石出了。这个BUG是因为架构部在对apollo client包进行定制化开发时不小心引入的，很显然没有经过仔细测试，并且刚好在我们上线前一天发布到了中央仓库中，而公司基础组件库的版本是通过super-pom方式统一维护的，业务无感知。

#### 解决方案

为了快速验证YGC耗时过长是因为此问题导致的，我们在一台服务器上直接用旧版本的apollo client 包进行了替换，然后重启了服务，观察了将近20分钟，YGC恢复正常。
最后，我们通知架构部修复BUG，重新发布了super-pom，彻底解决了这个问题。
02 YGC的相关知识点总结
通过上面这个案例，可以看到YGC问题其实比较难排查。相比FGC或者OOM，YGC的日志很简单，只知道新生代内存的变化和耗时，同时dump出来的堆内存必须要仔细排查才行。

另外，如果不清楚YGC的流程，排查起来会更加困难。这里，我对YGC相关的知识点再做下梳理，方便大家更全面的理解YGC。

### YGC的相关知识点总结

#### 5个问题重新认识新生代

![image-20230719220653122](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220653122.png)

YGC 在新生代中进行，首先要清楚新生代的堆结构划分。新生代分为Eden区和两个Survivor区，其中Eden:from:to = 8:1:1 (比例可以通过参数 –XX:SurvivorRatio 来设定 )，这是最基本的认识。

**为什么会有新生代？**

如果不分代，所有对象全部在一个区域，每次GC都需要对全堆进行扫描，存在效率问题。分代后，可分别控制回收频率，并采用不同的回收算法，确保GC性能全局最优。

**为什么新生代会采用复制算法？**

新生代的对象朝生夕死，大约90%的新建对象可以被很快回收，复制算法成本低，同时还能保证空间没有碎片。虽然标记整理算法也可以保证没有碎片，但是由于新生代要清理的对象数量很大，将存活的对象整理到待清理对象之前，需要大量的移动操作，时间复杂度比复制算法高。

**为什么新生代需要两个Survivor区？**

为了节省空间考虑，如果采用传统的复制算法，只有一个Survivor区，则Survivor区大小需要等于Eden区大小，此时空间消耗是8 * 2，而两块Survivor可以保持新对象始终在Eden区创建，存活对象在Survivor之间转移即可，空间消耗是8+1+1，明显后者的空间利用率更高。

**新生代的实际可用空间是多少？**

YGC后，总有一块Survivor区是空闲的，因此新生代的可用内存空间是90%。在YGC的log中或者通过 jmap -heap pid 命令查看新生代的空间时，如果发现capacity只有90%，不要觉得奇怪。

**Eden区是如何加速内存分配的？**

HotSpot虚拟机使用了两种技术来加快内存分配。分别是bump-the-pointer和TLAB（Thread Local Allocation Buffers）。

由于Eden区是连续的，因此bump-the-pointer在对象创建时，只需要检查最后一个对象后面是否有足够的内存即可，从而加快内存分配速度。

TLAB技术是对于多线程而言的，在Eden中为每个线程分配一块区域，减少内存分配时的锁冲突，加快内存分配速度，提升吞吐量。

#### 新生代的4种回收器

![image-20230719220705722](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220705722.png)

SerialGC（串行回收器），最古老的一种，单线程执行，适合单CPU场景。

ParNew（并行回收器），将串行回收器多线程化，适合多CPU场景，需要搭配老年代CMS回收器一起使用。

ParallelGC（并行回收器），和ParNew不同点在于它关注吞吐量，可设置期望的停顿时间，它在工作时会自动调整堆大小和其他参数。

G1（Garage-First回收器），JDK 9及以后版本的默认回收器，兼顾新生代和老年代，将堆拆成一系列Region，不要求内存块连续，新生代仍然是并行收集。

上述回收器均采用复制算法，都是独占式的，执行期间都会Stop The World.

#### YGC的触发时机

当Eden区空间不足时，就会触发YGC。结合新生代对象的内存分配看下详细过程：

1、新对象会先尝试在栈上分配，如果不行则尝试在TLAB分配，否则再看是否满足大对象条件要在老年代分配，最后才考虑在Eden区申请空间。

2、如果Eden区没有合适的空间，则触发YGC。

3、YGC时，对Eden区和From Survivor区的存活对象进行处理，如果满足动态年龄判断的条件或者To Survivor区空间不够则直接进入老年代，如果老年代空间也不够了，则会发生promotion failed，触发老年代的回收。否则将存活对象复制到To Survivor区。

4、此时Eden区和From Survivor区的剩余对象均为垃圾对象，可直接抹掉回收。

此外，老年代如果采用的是CMS回收器，为了减少CMS Remark阶段的耗时，也有可能会触发一次YGC，这里不作展开。

#### YGC的执行过程

YGC采用的复制算法，主要分成以下两个步骤：

> 1、查找GC Roots，将其引用的对象拷贝到S1区
> 2、递归遍历第1步的对象，拷贝其引用的对象到S1区或者晋升到Old区

上述整个过程都是需要暂停业务线程的（STW），不过ParNew等新生代回收器可以多线程并行执行，提高处理效率。
YGC通过可达性分析算法，从GC Root（可达对象的起点）开始向下搜索，标记出当前存活的对象，那么剩下未被标记的对象就是需要回收的对象。

![image-20230719220713899](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220713899.png)

可作为YGC时GC Root的对象包括以下几种：

> 1、虚拟机栈中引用的对象
> 2、方法区中静态属性、常量引用的对象
> 3、本地方法栈中引用的对象
> 4、被Synchronized锁持有的对象
> 5、记录当前被加载类的SystemDictionary
> 6、记录字符串常量引用的StringTable
> 7、存在跨代引用的对象
> 8、和GC Root处于同一CardTable的对象

其中1-3是大家容易想到的，而4-8很容易被忽视，却极有可能是分析YGC问题时的线索入口。

另外需要注意的是，针对下图中跨代引用的情况，老年代的对象A也必须作为GC Root的一部分，但是如果每次YGC时都去扫描老年代，肯定存在效率问题。在HotSpot JVM，引入卡表（Card Table）来对跨代引用的标记进行加速。

![image-20230719220721604](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220721604.png)

Card Table，简单理解是一种空间换时间的思路，因为存在跨代引用的对象大概占比不到1%，因此可将堆空间划分成大小为512字节的卡页，如果卡页中有一个对象存在跨代引用，则可以用1个字节来标识该卡页是dirty状态，卡页状态进一步通过写屏障技术进行维护。

遍历完GC Roots后，便能够找出第一批存活的对象，然后将其拷贝到S1区。接下来，就是一个递归查找和拷贝存活对象的过程。

S1区为了方便维护内存区域，引入了两个指针变量：_saved_mark_word和_top，其中_saved_mark_word表示当前遍历对象的位置，_top表示当前可分配内存的位置，很显然，_saved_mark_word到_top之间的对象都是已拷贝但未扫描的对象。

![image-20230719220735174](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220735174.png)

贝到S1区，_top也会往前移动，直到_saved_mark_word追上_top，说明S1区所有对象都已经遍历完成。

有一个细节点需要注意的是：拷贝对象的目标空间不一定是S1区，也可能是老年代。如果一个对象的年龄（经历的YGC次数）满足动态年龄判定条件便直接晋升到老年代中。对象的年龄保存在Java对象头的mark word数据结构中（如果大家对Java并发锁熟悉，肯定了解这个数据结构，不熟悉的建议查阅资料了解下，这里不做展开）。

### 最后的话

这篇文章通过线上案例分析并结合原理讲解，详细介绍了YGC的相关知识。从YGC实战角度出发，再简单总结一下：
1、首先要清楚YGC的执行原理，比如年轻代的堆内存结构、Eden区的内存分配机制、GC Roots扫描、对象拷贝过程等。
2、YGC的核心步骤是标注和复制，绝部分YGC问题都集中在这两步，因此可以结合YGC日志和堆内存变化情况逐一排查，同时dump的堆内存文件需要仔细分析。



## 4. 一次大量 JVM Native 内存泄露的排查分析（64M 问题）【难】

我们有一个线上的项目，刚启动完就占用了使用 top 命令查看 RES 占用了超过 1.5G，这明显不合理，于是进行了一些分析找到了根本的原因，下面是完整的分析过程，希望对你有所帮助。

会涉及到下面这些内容

- Linux 经典的 64M 内存问题
- 堆内存分析、Native 内存分析的基本套路
- tcmalloc、jemalloc 在 native 内存分析中的使用
- finalize 原理
- hibernate 毁人不倦

### 现象

程序启动的参数

```shell
shell复制代码ENV=FAT java
-Xms1g -Xmx1g 
-XX:MetaspaceSize=120m 
-XX:MaxMetaspaceSize=400m 
-XX:+UseConcMarkSweepGC  
-jar 
EasiCareBroadCastRPC.jar
```

启动后内存占用如下，惊人的 1.5G，Java 是内存大户，但是你也别这么玩啊。

![image-20230719220743330](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220743330.png)

下面是愉快的分析过程。

### 柿子先挑软的捏

先通过 jcmd 或者 jmap 查看堆内存是否占用比较高，如果是这个问题，那很快就可以解决了。

可以看到堆内存占用 216937K + 284294K = 489.48M，Metaspace 内存虽然不属于 Java 堆，这里也显示了出来占用 80M+，这两部分加起来，远没有到 1.5G。

![image-20230719220750568](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220750568.png)

那剩下的内存去了哪里？到这里，已经可以知道可能是堆以外的部分占用了内存，接下来就是开始使用 `NativeMemoryTracking` 来进行下一步分析。

### NativeMemoryTracking 使用

如果要跟踪其它部分的内存占用，需要通过 `-XX:NativeMemoryTracking` 来开启这个特性

```ini
java -XX:NativeMemoryTracking=[off | summary | detail]
```

加入这个启动参数，重新启动进程，随后使用 jcmd 来打印相关的信息。

```ini
$ jcmd `jps | grep -v Jps | awk '{print $1}'` VM.native_memory detail

Total: reserved=2656938KB, committed=1405158KB
-                 Java Heap (reserved=1048576KB, committed=1048576KB)
                            (mmap: reserved=1048576KB, committed=1048576KB)

-                     Class (reserved=1130053KB, committed=90693KB)
                            (classes #15920)
                            (malloc=1605KB #13168)
                            (mmap: reserved=1128448KB, committed=89088KB)

-                    Thread (reserved=109353KB, committed=109353KB)
                            (thread #107)
                            (stack: reserved=108884KB, committed=108884KB)
                            (malloc=345KB #546)
                            (arena=124KB #208)

-                      Code (reserved=257151KB, committed=44731KB)
                            (malloc=7551KB #9960)
                            (mmap: reserved=249600KB, committed=37180KB)

-                        GC (reserved=26209KB, committed=26209KB)
                            (malloc=22789KB #306)
                            (mmap: reserved=3420KB, committed=3420KB)

-                  Compiler (reserved=226KB, committed=226KB)
                            (malloc=95KB #679)
                            (arena=131KB #7)

-                  Internal (reserved=15063KB, committed=15063KB)
                            (malloc=15031KB #20359)
                            (mmap: reserved=32KB, committed=32KB)

-                    Symbol (reserved=22139KB, committed=22139KB)
                            (malloc=18423KB #196776)
                            (arena=3716KB #1)
```

很失望，这里面显示的所有的部分，看起来都很正常，没有特别大异常占用的情况，到这里我们基本上可以知道是不受 JVM 管控的 native 内存出了问题，那要怎么分析呢？

### pmap 初步查看

通过 pmap 我们可以查看进程的内存分布，可以看到有大量的 64M 内存区块区域，这部分是 linux 内存 ptmalloc 的典型现象，这个问题在之前的一篇「一次 Java 进程 OOM 的排查分析（glibc 篇）」已经介绍过了，详见：[juejin.cn/post/685457…](https://juejin.cn/post/6854573220733911048)

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/90a10f614a004a4f93fdcf6cc1a4c912~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

那这 64M 的内存区域块，是不是在上面 NMT 统计的内存区域里呢？

NMT 工具的地址输出 detail 模式会把每个区域的起始结束地址输出出来，如下所示。

![image-20230719220805768](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220805768.png)

写一个简单的代码（自己正则搞一下就行了）就可以将 pmap、nmt 两部分整合起来，看看真正的堆、栈、GC 等内存占用分布在内存地址空间的哪一个部分。

可以看到大量 64M 部分的内存区域不属于任何 NMT 管辖的部分。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf756676bbcc46d0b7b1cbe2e4a671d5~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

### tcmalloc、jemalloc 来救场

我们可以通过 tcmalloc 或者 jemalloc 可以做 native 内存分配的追踪，它们的原理都是 hook 系统 malloc、free 等内存申请释放函数的实现，增加 profile 的逻辑。

下面以 tcmalloc 为例。

从源码编译 tcmalloc（[github.com/gperftools/…](https://link.juejin.cn?target=http%3A%2F%2Fgithub.com%2Fgperftools%2Fgperftools%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E9%80%9A%E8%BF%87) `LD_PRELOAD` 来 hook 内存分配释放的函数。

```ini
HEAPPROFILE=./heap.log 
HEAP_PROFILE_ALLOCATION_INTERVAL=104857600 
LD_PRELOAD=./libtcmalloc_and_profiler.so
java -jar xxx ...
```

启动过程中就会看到生成了很多内存 dump 的分析文件，接下来使用 pprof 将 heap 文件转为可读性比较好的 pdf 文件。

```css
pprof --pdf /path/to/java heap.log.xx.heap > test.pdf
```

内存申请的链路如下图所示。

![image-20230719220819176](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220819176.png)

可以看到绝大部分的内存申请都耗在了 `Java_java_util_zip_Inflater_inflateBytes`，jar 包本质就是一个 zip 包， 在读取 jar 包文件过程中大量使用了 jni 中的 cpp 代码来处理，这里面大量申请释放了内存。

### 不用改代码的解决方式

既然是因为读取 jar 包这个 zip 文件导致的内存疯长，那我不用 `java -jar`，直接把原 jar 包解压，然后用 `java -cp . AppMain` 来启动是不是可以避免这个问题呢？因为我们项目因为历史原因是使用 shade 的方式，里面已经没有任何 jar 包了，全是 class 文件。奇迹出现了，不用 jar 包启动，RES 占用只有 400M，神奇不神奇！

![image-20230719220826689](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220826689.png)

到这里，我们更加确定是 jar 包启动导致的问题，那为什么 jar 包启动会导致问题呢？

### 探究根本原因

通过 tcmalloc 可以看到大量申请释放内存的地方在 java.util.zip.Inflater 类，调用它的 end 方法会释放 native 的内存。

我本以为是 end 方法没有调用导致的，这种的确是有可能的，java.util.zip.InflaterInputStream 类的 close 方法在一些场景下是不会调用 Inflater.end 方法，如下所示。

![image-20230719220832153](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220832153.png)

但是 Inflater 类有实现 finalize 方法，在 Inflater 对象不可达以后，JVM 会帮忙调用 Inflater 类的 finalize 方法，

```csharp
public class Inflater {

    public void end() {
        synchronized (zsRef) {
            long addr = zsRef.address();
            zsRef.clear();
            if (addr != 0) {
                end(addr);
                buf = null;
            }
        }
    }

    /**
     * Closes the decompressor when garbage is collected.
     */
    protected void finalize() {
        end();
    }

    private native static void initIDs();
    // ...
    private native static void end(long addr);
}
```

有两种可能性

- Inflater 因为被其它对象引用，没能释放，导致 finalize 方法不能被调用，内存自然没法释放
- Inflater 的 finalize 方法被调用，但是被 libc 的  ptmalloc 缓存，没能真正释放回操作系统

第二种可能性，我之前在另外一篇文章「一次 Java 进程 OOM 的排查分析（glibc 篇）」已经介绍过了，详见：[juejin.cn/post/685457…](https://juejin.cn/post/6854573220733911048) ，经验证，不是这个问题。

我们来看第一个可能性，通过 dump 堆内存来查看。果然，有 8 个 Inflater 对象还存活没能被 GC，除了被 JVM 内部的 java.lang.ref.Finalizer 引用，还有其它的引用，导致 Inflater 在 GC 时无法被回收。

![image-20230719220841665](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220841665.png)

那这些内存是不是真的跟 64M 的内存区块有关呢？空口无凭，我们来确认一把。Inflater 类有一个 zsRef 字段，其实它就是一个指针地址，我们看看未释放的 Inflater 的 zsRef 地址是不是位于我们所说的 64M 内存区块里。

```csharp
public class Inflater {
    private final ZStreamRef zsRef;
}

class ZStreamRef {
    private volatile long address;
    ZStreamRef (long address) {
        this.address = address;
    }

    long address() {
        return address;
    }

    void clear() {
        address = 0;
    }
}
    
```

通过一个 ZStreamRef 找到 address 等于 140686448095872，转为 16 进制为 0x7ff41dc37280，这个地址位于的虚拟地址空间在这里：

![image-20230719220849459](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220849459.png)

正是在我们所说的 64M 内存区块中。

![image-20230719220857273](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220857273.png)

如果你还不信，我们可以 dump 这块内存，我这里写了一个脚本

```bash
cat /proc/$1/maps | grep -Fv ".so" | grep " 0 " | awk '{print $1}' | grep $2 | ( IFS="-"
while read a b; do
dd if=/proc/$1/mem bs=$( getconf PAGESIZE ) iflag=skip_bytes,count_bytes \
skip=$(( 0x$a )) count=$(( 0x$b - 0x$a )) of="$1_mem_$a.bin"
done )
```

通过传入进程号和你想 dump 的内存起始地址，就可以把这块内存 dump 出来。

```bash
./dump.sh `pidof java` 7ff41c000000
```

执行上面的脚本，传入两个参数，一个是进程 id，一个地址，会生成一个 64M 的内存 dump 文件，通过 strings 查看。

```python
strings 6095_mem_7ff41c000000.bin
```

输出结果如下，满屏的都是类文件相关的信息。

![image-20230719220918587](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220918587.png)

到这里已经应该无需再证明什么了，剩下的就是分析的事了。

那到底是被谁引用的呢？展开引用链，看到出现了一堆 ClassLoader。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/91bf915aadb44609a4efa4afcbfd7dc5~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

### 一个意外的发现（与本问题关系不大，顺手解决一下）

这里出现了一个很奇怪的 nashorn 相关的 ClassLoader，众所周不知，nashorn 是处理 JavaScript 相关的逻辑的，那为毛这个项目会用到 nashorn 呢？经过仔细搜索，项目代码并没有使用。

那是哪个坑货中间件引入的呢？debug 一下马上就找到了原因，原来是臭名昭著的 log4j2，用了这么多年 log4j，头一回知道，原来 log4j2 是支持 javaScript、Groovy 等脚本语言的。

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Configuration status="debug" name="RoutingTest">
  <Scripts>
    <Script name="selector" language="javascript"><![CDATA[
            var result;
            if (logEvent.getLoggerName().equals("JavascriptNoLocation")) {
                result = "NoLocation";
            } else if (logEvent.getMarker() != null && logEvent.getMarker().isInstanceOf("FLOW")) {
                result = "Flow";
            }
            result;
            ]]></Script>
    <ScriptFile name="groovy.filter" path="scripts/filter.groovy"/>
  </Scripts>
</Configuration>
```

我们项目中并没有用到类似的特性（因为不知道），只能说真是无语，你好好的当一个工具人日志库不好吗，搞这么多花里胡哨的东西，肤浅！

代码在这里

![image-20230719220927678](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220927678.png)

这个问题我粗略看了一下，截止到官方最新版还没有一个开关可以关掉 ScriptEngine，不行就自己上，自己拉取项目中 log4j 对应版本的代码，做了修改，重新打包运行，

![image-20230719220934319](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220934319.png)

重新运行后 nashorn 部分的 ClassLoader 确实没有了，不过这里只是一个小插曲，native 内存占用的问题并没有解决。

### 凶手浮出水面

接下来我们就要找哪些代码在疯狂调用 `java.util.zip.Inflater.inflateBytes` 方法

使用 watch 每秒 jstack 一下线程，马上就看到了 hibernate 在疯狂的调用。

![image-20230719220940155](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220940155.png)

hibernate 是我们历史老代码遗留下来的，一直没有移除掉，看来还是踩坑了。

找到这个函数对应代码 `org.hibernate.jpa.boot.archive.internal.JarFileBasedArchiveDescriptor#visitArchive#146`

![image-20230719220945771](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220945771.png)

垃圾代码，`jarFile.getInputStream( zipEntry )` 生成了一个新的流但没有做关闭处理。

> 其实我也不知道，为啥 hibernate 要把我 jar 包中所有的类都扫描解析一遍，完全有毛病。

我们来把这段代码扒出来，写一个最简 demo。

```java
public class JarFileTest {
    public static void main(String[] args) throws IOException {
        new JarFileTest().process();
        System.in.read();
    }

    public static byte[] getBytesFromInputStream(InputStream inputStream) throws IOException {
        // 省略 read 的逻辑
        return result;
    }

    public void process() throws IOException {
        JarFile jarFile = null;
        try {
            jarFile = new JarFile("/data/dev/broadcast/EasiCareBroadCastRPC.jar");
            final Enumeration<? extends ZipEntry> zipEntries = jarFile.entries();
            while (zipEntries.hasMoreElements()) {
                final ZipEntry zipEntry = zipEntries.nextElement();
                if (zipEntry.isDirectory()) {
                    continue;
                }

                byte[] bytes = getBytesFromInputStream(jarFile.getInputStream(zipEntry));

                System.out.println("processing: " + zipEntry.getName() + "\t" + bytes.length);
            }
        } finally {
            try {
                if (jarFile != null) jarFile.close();
            } catch (Exception e) {
            }
        }
    }
}
```

运行上面的代码。

```ruby
javac JarFileTest.java
java -Xms1g -Xmx1g -XX:MetaspaceSize=120m -XX:MaxMetaspaceSize=400m -XX:+UseConcMarkSweepGC  -cp . JarFileTest
```

内存 RES 占用立马飙升到了 1.2G 以上，且无论如何 GC 都无法回收，但堆内存几乎等于 0。

RES 内存占用如下所示。![image-20230719220954750](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719220954750.png)

堆内存占用如下所示，经过 GC 以后新生代占用为 0，老年代占用为 275K

![image-20230719221002241](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719221002241.png)

全被 64M 内存占满。

![image-20230719221010204](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719221010204.png)

通过修改代码，将流关闭

```ini
while (zipEntries.hasMoreElements()) {
    final ZipEntry zipEntry = zipEntries.nextElement();
    if (zipEntry.isDirectory()) {
        continue;
    }

    InputStream is = jarFile.getInputStream(zipEntry);
    byte[] bytes = getBytesFromInputStream(is);

    System.out.println("processing: " + zipEntry.getName() + "\t" + bytes.length);
    try {
        is.close();
    } catch (Exception e) {

    }
}
```

再次测试，问题解决了，native 内存占用几乎消失了，接下来就是解决项目中的问题。一种是彻底移除 hibernate，将它替换为我们现在在用的 mybatis，这个我不会。我打算来改一下 hibernate 的源码。

### 尝试修改

修改这段代码（ps这里是不成熟的改动，close 都应该放 finally，多个 close 需要分别捕获异常，但是为了简单，这里先简化），加入 close 的逻辑。

![image-20230719221019223](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719221019223.png)

重新编译 hibernate，install 到本地，然后重新打包运行。此时 RES 占用从 1.5G 左右降到了 700 多 M。

![image-20230719221026648](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719221026648.png)

而且比较可喜的是，64M 区块的 native 内存占用非常非常小，这里 700M 内存有 448M 是 dirty 的 heap 区，这部分只是 JVM 预留的。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8ec2adcdbb1e4ab79d307d8f4069a878~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

到这里，我们基本上已经解决了这个问题。后面我去看了一下 hibernate 的源码，在新版本里面，已经解决了这个问题，但是我不打算升级了，干掉了事。

![image-20230719221036547](https://2290653824-github-io.oss-cn-hangzhou.aliyuncs.com/image-20230719221036547.png)



### 后记

因为不是本文的重点，文章涉及的一些工具的使用，我没有展开来聊，大家感兴趣可以自己搞定。

其实 native 内存泄露没有我们想象的那么复杂，可以通过 NMT、pmap、tcmalloc 逐步逐步进行分析，只要能复现，都不叫 bug。

最后珍爱生命，远离 hibernate。

有任何 JVM 相关的问题，欢迎加微或者公众号联系我，一起交流。
